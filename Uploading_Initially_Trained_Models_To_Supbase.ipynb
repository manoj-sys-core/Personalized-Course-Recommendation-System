{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbb140c",
   "metadata": {},
   "source": [
    "### Training and Uploading Models to Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc15915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import lzma\n",
    "import tempfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from supabase import create_client\n",
    "import sklearn\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bf5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "supabase = create_client(url, key)\n",
    "\n",
    "# Function for loading rating data\n",
    "\n",
    "def load_rating(batch_size=1000):\n",
    "\n",
    "    response = supabase.table(\"Ratings\").select(\"user\", count=\"exact\").execute()\n",
    "    total_rows = response.count\n",
    "\n",
    "    all_data = []\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size - 1, total_rows - 1)\n",
    "        batch_response = supabase.table(\"Ratings\").select(\"*\").range(start, end).execute()\n",
    "        \n",
    "        if batch_response.data:\n",
    "            all_data.extend(batch_response.data)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# Function for loading user model map data\n",
    "\n",
    "def load_user_model_map_by_userid(userid):\n",
    "    response = supabase.table(\"User_Model_Map\").select(\"*\").eq(\"userid\", userid).execute()\n",
    "\n",
    "    if response.data:\n",
    "        return pd.DataFrame(response.data)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "# Function for loading course data \n",
    "\n",
    "def load_course():\n",
    "    data = supabase.table(\"Course_Info\").select(\"*\").execute()\n",
    "    return pd.DataFrame(data.data)\n",
    "\n",
    "# Function for loading course bag of words data\n",
    "\n",
    "def load_course_BOW(batch_size=1000):\n",
    "\n",
    "    response = supabase.table(\"Course_BOW\").select(\"doc_id\", count=\"exact\").execute()\n",
    "    total_rows = response.count\n",
    "\n",
    "    all_data = []\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size - 1, total_rows - 1)\n",
    "        batch_response = supabase.table(\"Course_BOW\").select(\"*\").range(start, end).execute()\n",
    "        \n",
    "        if batch_response.data:\n",
    "            all_data.extend(batch_response.data)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# Function for loading course genre data\n",
    "\n",
    "def load_course_genre():\n",
    "    data = supabase.table(\"Course Genres\").select(\"*\").execute()\n",
    "    return pd.DataFrame(data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6626e",
   "metadata": {},
   "source": [
    "### Course Similarity Model (Initialization and Storing in Supabase Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4c504d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded Course Similarity to Supabase Storage'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def course_similarity_train():\n",
    "\n",
    "    bucket = \"course-recommendation-models\"\n",
    "    file_name = \"course_similarity_model.xz\"\n",
    "\n",
    "    course_df = load_course()\n",
    "    bow_df = load_course_BOW()\n",
    "\n",
    "    course_ids = course_df['COURSE_ID'].tolist()\n",
    "\n",
    "    def get_id_idx_dict(bow_df):\n",
    "\n",
    "        grouped_df = bow_df.groupby(['doc_index', 'doc_id']).max().reset_index(drop=False)\n",
    "        idx_id_dict = grouped_df['doc_id'].to_dict()\n",
    "        id_idx_dict = {v: k for k, v in idx_id_dict.items()}\n",
    "        return id_idx_dict\n",
    "    \n",
    "    id_idx_dict = get_id_idx_dict(bow_df)\n",
    "\n",
    "\n",
    "    bows_df = bow_df[['doc_id', 'token', 'bow']]\n",
    "    dtm = bows_df.pivot_table(index='doc_id', columns='token', values='bow', fill_value=0)\n",
    "\n",
    "    dtm = dtm.reindex(course_ids).fillna(0)\n",
    "    similarity_matrix = cosine_similarity(dtm)\n",
    "\n",
    "    obj = {\n",
    "        \"similarity_matrix\": similarity_matrix,\n",
    "        \"id_idx_dict\": id_idx_dict\n",
    "    }\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xz\")\n",
    "    tmp.close()\n",
    "\n",
    "    try:\n",
    "\n",
    "        with lzma.open(tmp.name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "        existing_files = [file[\"name\"] for file in supabase.storage.from_(bucket).list()]\n",
    "\n",
    "        with open(tmp.name, \"rb\") as f:\n",
    "\n",
    "            if file_name in existing_files:\n",
    "                supabase.storage.from_(bucket).update(file_name, f)\n",
    "                status = \"✅ Trained and Updated Course Similarity on Supabase Storage\"\n",
    "            else:\n",
    "                f.seek(0)\n",
    "                supabase.storage.from_(bucket).upload(file_name, f)\n",
    "                status = \"✅ Trained and Uploaded Course Similarity to Supabase Storage\"\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"❌ Error during training or upload: {e}\"\n",
    "    finally:\n",
    "        os.remove(tmp.name)\n",
    "\n",
    "    return status\n",
    "\n",
    "course_similarity_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e729606",
   "metadata": {},
   "source": [
    "### User Profile Model (Initialization and Storing in Supabase Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e08a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded User Profiles to Supabase'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def user_profile_train():\n",
    "\n",
    "    bucket = \"course-recommendation-models\"\n",
    "    file_name = \"user_profile_matrix.xz\"\n",
    "\n",
    "    users_df = load_rating()\n",
    "    users_df.columns = ['User_ID', 'COURSE_ID', 'Rating']\n",
    "    course_genres_df = load_course_genre()\n",
    "\n",
    "    user_course_rating = users_df.pivot_table(index='User_ID', columns='COURSE_ID', values='Rating', fill_value=0.0)\n",
    "\n",
    "    course_ids = course_genres_df['COURSE_ID'].values\n",
    "    course_genres_matrix = course_genres_df.iloc[:, 2:].astype(float).values\n",
    "    user_course_rating = user_course_rating.reindex(columns=course_ids, fill_value=0.0).astype(float)\n",
    "    user_profiles = np.dot(user_course_rating.values, course_genres_matrix)\n",
    "\n",
    "    profile_df = pd.DataFrame(\n",
    "        user_profiles,\n",
    "        columns=course_genres_df.columns[2:]\n",
    "    )\n",
    "    profile_df.insert(0, 'User_ID', user_course_rating.index)\n",
    "    profile_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xz\")\n",
    "    tmp.close()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        with lzma.open(tmp.name, \"wb\") as f:\n",
    "            pickle.dump(profile_df, f)\n",
    "\n",
    "        existing_files = [file[\"name\"] for file in supabase.storage.from_(bucket).list()]\n",
    "\n",
    "        with open(tmp.name, \"rb\") as f:\n",
    "\n",
    "            if file_name in existing_files:\n",
    "                supabase.storage.from_(bucket).update(file_name, f)\n",
    "                status = \"✅ Trained and Updated User Profiles on Supabase\"\n",
    "            else:\n",
    "                f.seek(0)\n",
    "                supabase.storage.from_(bucket).upload(file_name, f)\n",
    "                status = \"✅ Trained and Uploaded User Profiles to Supabase\"\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"❌ Error during training/upload: {e}\"\n",
    "    finally:\n",
    "        os.remove(tmp.name)\n",
    "\n",
    "    return status\n",
    "\n",
    "user_profile_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dc366",
   "metadata": {},
   "source": [
    "### Clustering with and without PCA Model (Initialization and Storing in Supabase Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(user_features_df, expected_variance = 90):\n",
    "\n",
    "    expected_variance = expected_variance / 100\n",
    "    n_com = 0\n",
    "\n",
    "    for n_components in range(1, user_features_df.shape[1]):\n",
    "        n_com = n_components\n",
    "        pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "        transformed_matrix = pca.fit_transform(user_features_df)\n",
    "        if (sum(pca.explained_variance_ratio_) >= expected_variance): break\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed_matrix)\n",
    "    transformed_df.columns = [f\"PC_{i}\" for i in range(n_com)]\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46b6adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded Clustering to Supabase'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kMeans_train(kMeans_model , n_clusters=25):\n",
    "\n",
    "    bucket = \"course-recommendation-models\"\n",
    "\n",
    "    if kMeans_model == 'Clustering with PCA':\n",
    "        file_name = \"kMeans_PCA_model.xz\"\n",
    "    else:\n",
    "        file_name = \"kMeans_model.xz\"\n",
    "    \n",
    "    rating_df = load_rating()\n",
    "    course_genres_df = load_course_genre()\n",
    "\n",
    "    course_ids = course_genres_df['COURSE_ID'].values\n",
    "    genre_cols = course_genres_df.columns[2:]\n",
    "    course_genres_matrix = course_genres_df.iloc[:, 2:].astype(float).to_numpy()\n",
    "\n",
    "    user_course_rating = (\n",
    "        rating_df.pivot(index='user', columns='item', values='rating')\n",
    "        .reindex(columns=course_ids, fill_value=0.0)\n",
    "        .fillna(0.0)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    user_course_rating = user_course_rating.sort_index()\n",
    "    \n",
    "    user_profile_matrix = np.dot(user_course_rating.values, course_genres_matrix)\n",
    "    profile_df = pd.DataFrame(user_profile_matrix, columns=genre_cols)\n",
    "    profile_df['User_ID'] = user_course_rating.index.values\n",
    "\n",
    "    profile_df = profile_df[['User_ID'] + genre_cols.tolist()]\n",
    "    \n",
    "    feature_names = list(genre_cols)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    profile_df[feature_names] = scaler.fit_transform(profile_df[feature_names])\n",
    "\n",
    "    user_ids2_idx = profile_df[['User_ID']]\n",
    "    user_features_df = profile_df.drop(columns=['User_ID'])\n",
    "\n",
    "    if kMeans_model == 'Clustering with PCA':\n",
    "        user_features_df = do_PCA(user_features_df, 90)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(user_features_df)\n",
    "\n",
    "    user_cluster_label = kmeans.labels_\n",
    "    user_cluster_label_df = pd.DataFrame(user_cluster_label)\n",
    "    user_cluster_label_df = pd.merge(user_ids2_idx, user_cluster_label_df, left_index=True, right_index=True)\n",
    "    user_cluster_label_df.columns = ['user', 'cluster']\n",
    "    \n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xz\")\n",
    "    tmp.close()\n",
    "    try:\n",
    "\n",
    "        with lzma.open(tmp.name, \"wb\") as f:\n",
    "            pickle.dump(user_cluster_label_df, f)\n",
    "\n",
    "        existing_files = [file[\"name\"] for file in supabase.storage.from_(bucket).list()]\n",
    "\n",
    "        with open(tmp.name, \"rb\") as f:\n",
    "\n",
    "            if file_name in existing_files:\n",
    "                supabase.storage.from_(bucket).update(file_name, f)\n",
    "                status = f\"✅ Trained and Updated {kMeans_model} on Supabase\"\n",
    "            else:\n",
    "                f.seek(0)\n",
    "                supabase.storage.from_(bucket).upload(file_name, f)\n",
    "                status = f\"✅ Trained and Uploaded {kMeans_model} to Supabase\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        status = f\"❌ Error during training/upload: {e}\"\n",
    "    finally:\n",
    "        os.remove(tmp.name)\n",
    "\n",
    "    return status\n",
    "\n",
    "kMeans_train('Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da2858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded Clustering with PCA to Supabase'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kMeans_train(\"Clustering with PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef24f42",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering Model (Initialization and Storing in Supabase Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f1cb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncf_create(n_users: int, n_items: int,                           \n",
    "               latent_dim_mf: int = 32, latent_dim_mlp: int = 32,   \n",
    "               reg_mf: int = 0, reg_mlp: int = 0.001,                \n",
    "               dense_layers: list = [16, 8, 4],                       \n",
    "               reg_layers: list = [0.01, 0.01, 0.01],                    \n",
    "               activation_dense: str = 'relu'                     \n",
    ") -> keras.Model:\n",
    "\n",
    "    user = keras.Input(shape=(), dtype='int32', name='user_id')\n",
    "    item = keras.Input(shape=(), dtype='int32', name='item_id')\n",
    "\n",
    "    mf_user_embedding = keras.layers.Embedding(input_dim = n_users,\n",
    "                                  output_dim = latent_dim_mf,\n",
    "                                  name = 'mf_user_embedding',\n",
    "                                  embeddings_initializer = 'RandomNormal',\n",
    "                                  embeddings_regularizer = keras.regularizers.l2(reg_mf)\n",
    "                                 )\n",
    "    \n",
    "    mf_item_embedding = keras.layers.Embedding(input_dim = n_items,\n",
    "                                  output_dim = latent_dim_mf,\n",
    "                                  name = 'mf_item_embedding',\n",
    "                                  embeddings_initializer = 'RandomNormal',\n",
    "                                  embeddings_regularizer = keras.regularizers.l2(reg_mf)\n",
    "                                 )\n",
    "\n",
    "    mlp_user_embedding = keras.layers.Embedding(input_dim = n_users,\n",
    "                                   output_dim = latent_dim_mlp,\n",
    "                                   name = 'mlp_user_embedding',\n",
    "                                   embeddings_initializer = 'RandomNormal',\n",
    "                                   embeddings_regularizer = keras.regularizers.l2(reg_mlp)\n",
    "                                  )\n",
    "    mlp_item_embedding = keras.layers.Embedding(input_dim = n_items,\n",
    "                                  output_dim = latent_dim_mlp,\n",
    "                                  name = 'mlp_item_embedding',\n",
    "                                  embeddings_initializer = 'RandomNormal',\n",
    "                                  embeddings_regularizer = keras.regularizers.l2(reg_mlp)\n",
    "                                 )\n",
    "\n",
    "    mf_user_latent = keras.layers.Flatten()(mf_user_embedding(user))\n",
    "    mf_item_latent = keras.layers.Flatten()(mf_item_embedding(item))\n",
    "\n",
    "    mlp_user_latent = keras.layers.Flatten()(mlp_user_embedding(user))\n",
    "    mlp_item_latent = keras.layers.Flatten()(mlp_item_embedding(item))\n",
    "\n",
    "    mf_cat_latent = keras.layers.Multiply()([mf_user_latent, mf_item_latent])\n",
    "    mlp_cat_latent = keras.layers.Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "    mlp_vector = mlp_cat_latent\n",
    "    for i in range(len(dense_layers)):\n",
    "        layer = keras.layers.Dense(\n",
    "                      units = dense_layers[i],\n",
    "                      activation = activation_dense,\n",
    "                      activity_regularizer = keras.regularizers.l2(reg_layers[i]),\n",
    "                      name = 'layer%d' % i,\n",
    "                     )\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "    \n",
    "    predict_layer = keras.layers.Concatenate()([mf_cat_latent, mlp_vector])\n",
    "    result = keras.layers.Dense(\n",
    "                   units = 1, \n",
    "                   activation = 'sigmoid',\n",
    "                   kernel_initializer = 'lecun_uniform',\n",
    "                   name = 'interaction' \n",
    "                  )\n",
    "\n",
    "    output = result(predict_layer)\n",
    "\n",
    "    model = keras.Model(inputs = [user, item],\n",
    "                  outputs = [output]\n",
    "                 )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9610e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncf_data_prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df_uim = (df.pivot(index='user', columns='item', values='rating')\n",
    "            .reset_index()\n",
    "            .rename_axis(columns=None, index=None)\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "    old_cols = df_uim.columns[1:]\n",
    "    new_cols = [i for i in range(len(old_cols))]\n",
    "    items_id2idx = {old_cols[i]: new_cols[i] for i in range(len(old_cols))}\n",
    "    df_uim = df_uim.rename(mapper=items_id2idx, axis=1)\n",
    "\n",
    "    original_user_ids = df_uim['user'].tolist()\n",
    "    user_id2idx = {user_id: idx for idx, user_id in enumerate(original_user_ids)}\n",
    "    df_uim['user'] = df_uim['user'].map(user_id2idx)\n",
    "\n",
    "    df_train = (pd.DataFrame(df_uim.iloc[:, 1:].stack())\n",
    "                .reset_index()\n",
    "                .sort_values(by='level_0')\n",
    "                .rename({'level_0': 'user_id', 'level_1': 'item_id', 0: 'interaction'}, axis=1)\n",
    "               )\n",
    "    df_train['interaction'] = df_train['interaction'].apply(lambda x: 1.0 if x > 0 else 0.0)\n",
    "\n",
    "    df_train['user_id'] = df_train['user_id'].astype('int')\n",
    "    df_train['item_id'] = df_train['item_id'].astype('int')\n",
    "    df_train['interaction'] = df_train['interaction'].astype('float32')\n",
    "\n",
    "    return df_train.sort_values(by=['user_id', 'item_id']), user_id2idx, items_id2idx\n",
    "\n",
    "\n",
    "def ncf_build_train_val_dataset(df: pd.DataFrame, val_split: float = 0.1, batch_size: int = 512, rs: int = 42):\n",
    "    \n",
    "    df['user_id'] = df['user_id'].astype('int32')\n",
    "    df['item_id'] = df['item_id'].astype('int32')\n",
    "    df['interaction'] = df['interaction'].astype('float32')\n",
    "\n",
    "    if rs:\n",
    "        df = df.sample(frac=1, random_state=rs).reset_index(drop=True)\n",
    "\n",
    "    n_val = round(len(df) * val_split)\n",
    "    x = {\n",
    "        'user_id': df['user_id'].values,\n",
    "        'item_id': df['item_id'].values\n",
    "    }\n",
    "    y = df['interaction'].values\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "    ds_val = ds.take(n_val).batch(batch_size)\n",
    "    ds_train = ds.skip(n_val).batch(batch_size)\n",
    "\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc2fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncf_train_model(ds_train, ds_val, n_epochs: int = 10):\n",
    "\n",
    "    n_users, n_items = (load_rating()\n",
    "                        .pivot(index='user', columns='item', values='rating')\n",
    "                        .reset_index()\n",
    "                        .rename_axis(index=None, columns=None)\n",
    "                        .shape)\n",
    "    \n",
    "    ncf_model = ncf_create(n_users=n_users, n_items=n_items)\n",
    "    ncf_model.compile(optimizer = \"adam\",\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics = [\n",
    "                                tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "                                tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "                                tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "                                tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "                                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                                tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                                tf.keras.metrics.Recall(name=\"recall\"),\n",
    "                                tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                                ]\n",
    "                    )\n",
    "\n",
    "    ncf_model._name = 'neural_collaborative_filtering'\n",
    "    ncf_hist = ncf_model.fit(x=ds_train, \n",
    "                             validation_data=ds_val,\n",
    "                             epochs=n_epochs,\n",
    "                             verbose=1\n",
    "                            )\n",
    "    return ncf_model, ncf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230e9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(bucket: str, file_path: str) -> bool:\n",
    "    \n",
    "    try:\n",
    "        if \"/\" in file_path:\n",
    "            folder, file_name = file_path.rsplit(\"/\", 1)\n",
    "        else:\n",
    "            folder, file_name = \"\", file_path\n",
    "            \n",
    "        file_list = supabase.storage.from_(bucket).list(path=folder)\n",
    "\n",
    "        if not isinstance(file_list, list):\n",
    "            return False\n",
    "\n",
    "        file_names = [file['name'] for file in file_list]\n",
    "        return file_name in file_names\n",
    "\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "def load_model_metadata_from_supabase(bucket, file_name):\n",
    "    try:\n",
    "        res = supabase.storage.from_(bucket).download(file_name)\n",
    "        raw = res if isinstance(res, bytes) else getattr(res, \"data\", None)\n",
    "        buf = io.BytesIO(raw)\n",
    "        with lzma.open(buf, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        return obj.get(\"trained_on\", pd.DataFrame()), obj.get(\"user_id2idx\", {})\n",
    "\n",
    "    except Exception:\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def upload_model_and_mappings_to_supabase(model, df, user_id2idx, item_id2idx, bucket, file_name):\n",
    "    status = \"\"\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".keras\", delete=False) as tmp_model:\n",
    "        model.save(tmp_model.name) \n",
    "        model_path = tmp_model.name\n",
    "\n",
    "    with open(model_path, \"rb\") as f_model:\n",
    "        model_binary = f_model.read()\n",
    "\n",
    "    obj = {\n",
    "        \"model\": model_binary,\n",
    "        \"trained_on\": df,\n",
    "        \"user_id2idx\": user_id2idx,\n",
    "        \"item_id2idx\": item_id2idx\n",
    "    }\n",
    "\n",
    "    tmp_bundle = tempfile.NamedTemporaryFile(delete=False, suffix=\".xz\")\n",
    "    tmp_bundle.close()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        with lzma.open(tmp_bundle.name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "        existing_files = [file[\"name\"] for file in supabase.storage.from_(bucket).list()]\n",
    "\n",
    "        with open(tmp_bundle.name, \"rb\") as f:\n",
    "\n",
    "            if file_name in existing_files:\n",
    "                supabase.storage.from_(bucket).update(file_name, f)\n",
    "                status = f\"✅ Trained and Updated NCF model to `{file_name}`\"\n",
    "            else:\n",
    "                f.seek(0)\n",
    "                supabase.storage.from_(bucket).upload(file_name, f)\n",
    "                status = f\"✅ Trained and Uploaded NCF model to `{file_name}`\"\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"❌ Error during saving or upload: {e}\"\n",
    "    finally:\n",
    "        os.remove(model_path)\n",
    "        os.remove(tmp_bundle.name)\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984f842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 49ms/step - accuracy: 0.9433 - auc: 0.7030 - fn: 101772.8438 - fp: 2623.6624 - loss: 0.3700 - precision: 0.2831 - recall: 0.0187 - tn: 1815456.6250 - tp: 2962.7554 - val_accuracy: 0.9531 - val_auc: 0.9174 - val_fn: 17403.0000 - val_fp: 2626.0000 - val_loss: 0.1307 - val_precision: 0.6978 - val_recall: 0.2584 - val_tn: 401060.0000 - val_tp: 6064.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 43ms/step - accuracy: 0.9579 - auc: 0.9371 - fn: 65778.8438 - fp: 13097.3809 - loss: 0.1157 - precision: 0.7405 - recall: 0.3485 - tn: 1804982.8750 - tp: 38956.7617 - val_accuracy: 0.9568 - val_auc: 0.9336 - val_fn: 14085.0000 - val_fp: 4372.0000 - val_loss: 0.1198 - val_precision: 0.6821 - val_recall: 0.3998 - val_tn: 399314.0000 - val_tp: 9382.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 47ms/step - accuracy: 0.9666 - auc: 0.9640 - fn: 49207.6445 - fp: 13148.7295 - loss: 0.0905 - precision: 0.8004 - recall: 0.5149 - tn: 1804931.5000 - tp: 55527.9570 - val_accuracy: 0.9577 - val_auc: 0.9331 - val_fn: 12591.0000 - val_fp: 5465.0000 - val_loss: 0.1209 - val_precision: 0.6656 - val_recall: 0.4635 - val_tn: 398221.0000 - val_tp: 10876.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 52ms/step - accuracy: 0.9763 - auc: 0.9804 - fn: 32992.7773 - fp: 10826.4014 - loss: 0.0664 - precision: 0.8623 - recall: 0.6705 - tn: 1807253.8750 - tp: 71742.8203 - val_accuracy: 0.9567 - val_auc: 0.9187 - val_fn: 11742.0000 - val_fp: 6734.0000 - val_loss: 0.1335 - val_precision: 0.6352 - val_recall: 0.4996 - val_tn: 396952.0000 - val_tp: 11725.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 65ms/step - accuracy: 0.9839 - auc: 0.9898 - fn: 21905.7285 - fp: 7790.1870 - loss: 0.0464 - precision: 0.9100 - recall: 0.7823 - tn: 1810290.1250 - tp: 82829.8750 - val_accuracy: 0.9550 - val_auc: 0.8953 - val_fn: 11512.0000 - val_fp: 7693.0000 - val_loss: 0.1571 - val_precision: 0.6085 - val_recall: 0.5094 - val_tn: 395993.0000 - val_tp: 11955.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 63ms/step - accuracy: 0.9892 - auc: 0.9942 - fn: 14609.0000 - fp: 5215.7505 - loss: 0.0322 - precision: 0.9423 - recall: 0.8546 - tn: 1812864.5000 - tp: 90126.6016 - val_accuracy: 0.9532 - val_auc: 0.8727 - val_fn: 11439.0000 - val_fp: 8556.0000 - val_loss: 0.1904 - val_precision: 0.5843 - val_recall: 0.5125 - val_tn: 395130.0000 - val_tp: 12028.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 64ms/step - accuracy: 0.9929 - auc: 0.9963 - fn: 9612.2393 - fp: 3450.6726 - loss: 0.0223 - precision: 0.9631 - recall: 0.9048 - tn: 1814629.6250 - tp: 95123.3594 - val_accuracy: 0.9516 - val_auc: 0.8522 - val_fn: 11450.0000 - val_fp: 9240.0000 - val_loss: 0.2304 - val_precision: 0.5653 - val_recall: 0.5121 - val_tn: 394446.0000 - val_tp: 12017.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 64ms/step - accuracy: 0.9951 - auc: 0.9973 - fn: 6612.6338 - fp: 2453.6240 - loss: 0.0162 - precision: 0.9745 - recall: 0.9349 - tn: 1815626.6250 - tp: 98122.9688 - val_accuracy: 0.9503 - val_auc: 0.8353 - val_fn: 11524.0000 - val_fp: 9705.0000 - val_loss: 0.2741 - val_precision: 0.5517 - val_recall: 0.5089 - val_tn: 393981.0000 - val_tp: 11943.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 66ms/step - accuracy: 0.9965 - auc: 0.9978 - fn: 4753.5889 - fp: 1879.3125 - loss: 0.0124 - precision: 0.9809 - recall: 0.9534 - tn: 1816201.0000 - tp: 99982.0156 - val_accuracy: 0.9493 - val_auc: 0.8233 - val_fn: 11539.0000 - val_fp: 10119.0000 - val_loss: 0.3189 - val_precision: 0.5410 - val_recall: 0.5083 - val_tn: 393567.0000 - val_tp: 11928.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m7509/7509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 70ms/step - accuracy: 0.9973 - auc: 0.9981 - fn: 3534.7256 - fp: 1464.1462 - loss: 0.0098 - precision: 0.9853 - recall: 0.9657 - tn: 1816616.1250 - tp: 101200.8750 - val_accuracy: 0.9482 - val_auc: 0.8144 - val_fn: 11565.0000 - val_fp: 10574.0000 - val_loss: 0.3634 - val_precision: 0.5295 - val_recall: 0.5072 - val_tn: 393112.0000 - val_tp: 11902.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded NCF model to `ncf_model.xz`'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NCF_train():\n",
    "    \n",
    "    bucket = \"course-recommendation-models\"\n",
    "    file_name = \"ncf_model.xz\"\n",
    "\n",
    "    def train_and_upload():\n",
    "\n",
    "        df = load_rating()\n",
    "        df_train, user_id2idx, item_id2idx = ncf_data_prep(df)\n",
    "        ds_train, ds_val = ncf_build_train_val_dataset(df=df_train, val_split=0.1, rs=42)\n",
    "\n",
    "        model, _ = ncf_train_model(ds_train=ds_train, ds_val=ds_val, n_epochs=10)\n",
    "\n",
    "        return upload_model_and_mappings_to_supabase(\n",
    "            model,\n",
    "            df,\n",
    "            user_id2idx=user_id2idx,\n",
    "            item_id2idx=item_id2idx,\n",
    "            bucket=bucket,\n",
    "            file_name=file_name\n",
    "        )\n",
    "\n",
    "    if check_file_exists(bucket, file_name):\n",
    "\n",
    "        ratings_df = load_rating()\n",
    "        trained_on, user_map = load_model_metadata_from_supabase(bucket, file_name)\n",
    "\n",
    "        new_users = set(ratings_df[\"user\"].unique())\n",
    "        old_users = set(user_map.keys())\n",
    "\n",
    "        if new_users.issubset(old_users) and ratings_df.shape == trained_on.shape :\n",
    "            return f\"✅ No new users. NCF model already up-to-date at `{file_name}`\"\n",
    "        else:\n",
    "            return train_and_upload()\n",
    "    else:\n",
    "        return train_and_upload()\n",
    "\n",
    "NCF_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b53fa6",
   "metadata": {},
   "source": [
    "### Regression and Classification with Embedding Features Model (Initialization and Storing in Supabase Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11813d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_create(n_user, n_item, n_user_latent_dim: int = 16, n_item_latent_dim: int = 16, reg_users: int = 1e-6, reg_items: int = 1e-6) -> keras.Model:\n",
    "\n",
    "    user_input = keras.layers.Input(shape=(), dtype='int32', name='user')\n",
    "    item_input = keras.layers.Input(shape=(), dtype='int32', name='item')\n",
    "\n",
    "    # USER\n",
    "    user_embedding = keras.layers.Embedding(input_dim=n_user,\n",
    "                    output_dim=n_user_latent_dim,\n",
    "                    name='user_embedding',\n",
    "                    embeddings_initializer=\"he_normal\",\n",
    "                    embeddings_regularizer=keras.regularizers.l2(reg_users)\n",
    "                    )(user_input)\n",
    "    \n",
    "    user_vec = keras.layers.Flatten(name='user_flat')(user_embedding)\n",
    "\n",
    "    user_bias = keras.layers.Embedding(input_dim=n_user,\n",
    "                    output_dim=1,\n",
    "                    name='user_bias',\n",
    "                    embeddings_initializer=\"he_normal\",\n",
    "                    embeddings_regularizer=keras.regularizers.l2(reg_users)\n",
    "                    )(user_input)\n",
    "    \n",
    "    user_model = keras.models.Model(inputs=user_input, outputs=user_vec)\n",
    "\n",
    "    # ITEM\n",
    "    item_embedding = keras.layers.Embedding(input_dim=n_item,\n",
    "                    output_dim=n_item_latent_dim,\n",
    "                    name='item_embedding',\n",
    "                    embeddings_initializer=\"he_normal\",\n",
    "                    embeddings_regularizer=keras.regularizers.l2(reg_items)\n",
    "                    )(item_input)\n",
    "\n",
    "    item_bias = keras.layers.Embedding(input_dim=n_user,\n",
    "                    output_dim=1,\n",
    "                    name='item_bias',\n",
    "                    embeddings_initializer=\"he_normal\",\n",
    "                    embeddings_regularizer=keras.regularizers.l2(reg_users)\n",
    "                    )(item_input)\n",
    "\n",
    "    merged = keras.layers.Dot(name='dot', normalize=True, axes=1)([user_embedding, item_embedding])\n",
    "    merged_dropout = keras.layers.Dropout(0.2)(merged)\n",
    "\n",
    "    #hidden layers\n",
    "    dense_1 = keras.layers.Dense(units=64, name='Dense_1')(merged_dropout)\n",
    "    do_1 = keras.layers.Dropout(0.2, name='Dropout_1')(dense_1)\n",
    "\n",
    "    dense_2 = keras.layers.Dense(units=32, name='Dense_2')(do_1)\n",
    "    do_2 = keras.layers.Dropout(0.2, name='Dropout_2')(dense_2)\n",
    "\n",
    "    dense_3 = keras.layers.Dense(units=16, name='Dense_3')(do_2)\n",
    "    do_3 = keras.layers.Dropout(0.2, name='Dropout_3')(dense_3)\n",
    "\n",
    "    dense_4 = keras.layers.Dense(units=8, name='Dense_4')(do_3)\n",
    "\n",
    "    result = keras.layers.Dense(1, name='rating', activation='relu')(dense_4)\n",
    "\n",
    "    model = keras.models.Model(inputs=[user_input, item_input], outputs=[result])\n",
    "    model._name = 'embedding_extraction_model'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53200632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_data_prep(raw_data):\n",
    "    \n",
    "    encoded_data = raw_data.copy()\n",
    "\n",
    "    user_id2idx = {x: i for i, x in enumerate(encoded_data[\"user\"].unique())}\n",
    "    item_id2idx = {x: i for i, x in enumerate(encoded_data[\"item\"].unique())}\n",
    "\n",
    "    encoded_data['user'] = encoded_data['user'].map(user_id2idx)\n",
    "    encoded_data['item'] = encoded_data['item'].map(item_id2idx)\n",
    "\n",
    "    return encoded_data, user_id2idx, item_id2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095d13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_ds_create(df, val_split = 0.1, batch_size = 512, rs = 42):\n",
    "    \n",
    "    if rs: \n",
    "        df = df.sample(frac=1, random_state=rs).reset_index(drop=True)\n",
    "\n",
    "    n_val = round(len(df) * val_split)\n",
    "\n",
    "    x = {\n",
    "        'user': df['user'].values,\n",
    "        'item': df['item'].values\n",
    "    }\n",
    "    y = df['rating'].values\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "    ds_val = ds.take(n_val).batch(batch_size)\n",
    "    ds_train = ds.skip(n_val).batch(batch_size)\n",
    "    \n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37658a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_train(ds_train, ds_val, num_users, num_items, epochs = 10, embedding_size = 16):\n",
    "\n",
    "    emb_model = emb_create(n_user=num_users, n_item=num_items, n_item_latent_dim=embedding_size, n_user_latent_dim=embedding_size)\n",
    "    emb_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    emb_model_hist = emb_model.fit(x = ds_train, validation_data = ds_val, epochs=epochs, verbose=1)\n",
    "\n",
    "    return emb_model, emb_model_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c2c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - loss: 1.8920 - mean_squared_error: 1.8919 - val_loss: 0.0487 - val_mean_squared_error: 0.0486\n",
      "Epoch 2/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.1912 - mean_squared_error: 0.1911 - val_loss: 0.0467 - val_mean_squared_error: 0.0465\n",
      "Epoch 3/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.1173 - mean_squared_error: 0.1171 - val_loss: 0.0445 - val_mean_squared_error: 0.0444\n",
      "Epoch 4/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0713 - mean_squared_error: 0.0711 - val_loss: 0.0438 - val_mean_squared_error: 0.0436\n",
      "Epoch 5/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0517 - mean_squared_error: 0.0515 - val_loss: 0.0437 - val_mean_squared_error: 0.0436\n",
      "Epoch 6/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0469 - mean_squared_error: 0.0467 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 7/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0460 - mean_squared_error: 0.0458 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 8/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0459 - mean_squared_error: 0.0457 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 9/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0458 - mean_squared_error: 0.0456 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 10/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0457 - mean_squared_error: 0.0455 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded Regression with Embedding Features to Supabase'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Embedding_train(model_name):\n",
    "\n",
    "    bucket = \"course-recommendation-models\"\n",
    "\n",
    "    if model_name == \"Regression with Embedding Features\":\n",
    "        file_name = \"regression_emb_model.xz\"\n",
    "    else:\n",
    "        file_name = \"classification_emb_model.xz\"\n",
    "\n",
    "    ratings_df = load_rating()\n",
    "\n",
    "    num_users = len(ratings_df['user'].unique())\n",
    "    num_items = len(ratings_df['item'].unique())\n",
    "\n",
    "    encoded_data, user_id2idx, course_id2idx = emb_data_prep(ratings_df)\n",
    "    ds_train, ds_val = emb_ds_create(encoded_data)\n",
    "\n",
    "    emb, _ = emb_train(ds_train=ds_train, ds_val=ds_val, num_users=num_users, num_items=num_items)\n",
    "\n",
    "    user_latent_features = emb.get_layer('user_embedding').get_weights()[0]\n",
    "    item_latent_features = emb.get_layer('item_embedding').get_weights()[0]\n",
    "\n",
    "    user_emb = pd.DataFrame(user_latent_features, columns=[f'User_Feature_{i}' for i in range(user_latent_features.shape[1])])\n",
    "    user_emb.insert(0, 'User_ID', list(user_id2idx.keys()))\n",
    "\n",
    "    item_emb = pd.DataFrame(item_latent_features, columns= [f'Course_Feature_{i}' for i in range(item_latent_features.shape[1])])\n",
    "    item_emb.insert(0, 'Course_ID', list(course_id2idx.keys()))\n",
    "\n",
    "    train_df = ratings_df.copy()\n",
    "\n",
    "    user_emb_train_merged = pd.merge(train_df, user_emb, how='left', left_on='user', right_on='User_ID').fillna(0)\n",
    "    merged_train_df = pd.merge(user_emb_train_merged, item_emb, how='left', left_on='item', right_on='Course_ID').fillna(0)\n",
    "\n",
    "    u_features = [f\"User_Feature_{i}\" for i in range(user_emb.shape[1] - 1)]\n",
    "    c_features = [f\"Course_Feature_{i}\" for i in range(item_emb.shape[1] - 1)]\n",
    "\n",
    "    user_train_embeddings = merged_train_df[u_features]\n",
    "    course_train_embeddings = merged_train_df[c_features]\n",
    "    ratings_train = merged_train_df['rating']\n",
    "\n",
    "    x_train = user_train_embeddings + course_train_embeddings.values\n",
    "    x_train.columns = [f\"Feature_{i}\" for i in range(item_emb.shape[1] - 1)]\n",
    "    y_train = ratings_train\n",
    "\n",
    "    if model_name == \"Regression with Embedding Features\":\n",
    "        rf_model = sklearn.ensemble.RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='log2',\n",
    "            max_depth=20,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "        y_train = label_encoder.fit_transform(y_train.values.ravel())\n",
    "        rf_model = sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            max_features='sqrt',\n",
    "            min_samples_split=2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    rf_model.fit(x_train, y_train)\n",
    "\n",
    "    obj = {\n",
    "        \"rf_model\": rf_model,\n",
    "        \"user_emb\": user_emb,\n",
    "        \"item_emb\": item_emb\n",
    "    }\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xz\")\n",
    "    tmp.close()\n",
    "    \n",
    "    try:\n",
    "\n",
    "        with lzma.open(tmp.name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "        existing_files = [file[\"name\"] for file in supabase.storage.from_(bucket).list()]\n",
    "\n",
    "        with open(tmp.name, \"rb\") as f:\n",
    "\n",
    "            if file_name in existing_files:\n",
    "                supabase.storage.from_(bucket).update(file_name, f)\n",
    "                status = f\"✅ Trained and Updated {model_name} on Supabase\"\n",
    "            else:\n",
    "                f.seek(0)\n",
    "                supabase.storage.from_(bucket).upload(file_name, f)\n",
    "                status = f\"✅ Trained and Uploaded {model_name} to Supabase\"\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"❌ Error during training/upload: {e}\"\n",
    "    finally:\n",
    "        os.remove(tmp.name)\n",
    "\n",
    "    return status\n",
    "\n",
    "\n",
    "Embedding_train(\"Regression with Embedding Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c87f4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 1.8199 - mean_squared_error: 1.8198 - val_loss: 0.0462 - val_mean_squared_error: 0.0461\n",
      "Epoch 2/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.1829 - mean_squared_error: 0.1827 - val_loss: 0.0455 - val_mean_squared_error: 0.0454\n",
      "Epoch 3/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.1134 - mean_squared_error: 0.1132 - val_loss: 0.0444 - val_mean_squared_error: 0.0442\n",
      "Epoch 4/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0783 - mean_squared_error: 0.0781 - val_loss: 0.0440 - val_mean_squared_error: 0.0438\n",
      "Epoch 5/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0574 - mean_squared_error: 0.0572 - val_loss: 0.0438 - val_mean_squared_error: 0.0436\n",
      "Epoch 6/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0493 - mean_squared_error: 0.0491 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 7/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0470 - mean_squared_error: 0.0468 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 8/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0464 - mean_squared_error: 0.0462 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 9/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0461 - mean_squared_error: 0.0459 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n",
      "Epoch 10/10\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0460 - mean_squared_error: 0.0458 - val_loss: 0.0437 - val_mean_squared_error: 0.0435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'✅ Trained and Uploaded Classification with Embedding Features to Supabase'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_train(\"Classification with Embedding Features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
